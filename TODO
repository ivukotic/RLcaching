To remember:
1. RESET returns initial observation
2. ACTOR decides what to do
3. action is sent and reward obtained together with the n+1 observation=> observation, reward, done, info = env.step(action)

ENVIRONMENT memorizes last decision concerning file and cleans based on that. For discreete decisions removes older files first.

### ENV
* Why peak at 0 in CHR?
* check what large environment does

### LRU actor


### DDQN 
Actor solves discrete cache env
* do we need episodes? The only use could be in initial training.
* Why replays at all? Simply tune up learning rate?
* no need to memorize next_state and done

### AC 
Solves continuous cache env